{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b0f414",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-27T14:07:11.323090Z",
     "iopub.status.busy": "2022-05-27T14:07:11.322490Z",
     "iopub.status.idle": "2022-05-27T14:07:11.536768Z",
     "shell.execute_reply": "2022-05-27T14:07:11.535881Z"
    },
    "papermill": {
     "duration": 0.222898,
     "end_time": "2022-05-27T14:07:11.539204",
     "exception": false,
     "start_time": "2022-05-27T14:07:11.316306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "import random\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "686bc17a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T14:07:11.548466Z",
     "iopub.status.busy": "2022-05-27T14:07:11.547530Z",
     "iopub.status.idle": "2022-05-27T14:07:11.551811Z",
     "shell.execute_reply": "2022-05-27T14:07:11.551051Z"
    },
    "papermill": {
     "duration": 0.010788,
     "end_time": "2022-05-27T14:07:11.553754",
     "exception": false,
     "start_time": "2022-05-27T14:07:11.542966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_key = '5qmflp3j50GXaiy2Jbr2IxzXPrEJabDdNaWNnZZvXr_gskKV3MunOwc9Gp2KLGDZwQIFQkzD0aUjS2s5blQg2xYNSBLTwY30s5X_nKfx8y1iePDi-mPrGo9co8OQYnYx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7fe4b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T14:07:11.562243Z",
     "iopub.status.busy": "2022-05-27T14:07:11.561869Z",
     "iopub.status.idle": "2022-05-27T14:07:11.565870Z",
     "shell.execute_reply": "2022-05-27T14:07:11.565029Z"
    },
    "papermill": {
     "duration": 0.010625,
     "end_time": "2022-05-27T14:07:11.567925",
     "exception": false,
     "start_time": "2022-05-27T14:07:11.557300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API constants, you shouldn't have to change these.\n",
    "API_HOST = 'https://api.yelp.com'\n",
    "SEARCH_PATH = '/v3/businesses/search'\n",
    "BUSINESS_PATH = '/v3/businesses/'  # Business ID will come after slash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2f8337",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T14:07:11.577022Z",
     "iopub.status.busy": "2022-05-27T14:07:11.576101Z",
     "iopub.status.idle": "2022-05-27T14:07:11.580859Z",
     "shell.execute_reply": "2022-05-27T14:07:11.580247Z"
    },
    "papermill": {
     "duration": 0.011461,
     "end_time": "2022-05-27T14:07:11.582819",
     "exception": false,
     "start_time": "2022-05-27T14:07:11.571358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "end_point = 'https://api.yelp.com/v3/businesses/search'\n",
    "headers = {'Authorization': 'bearer %s' % api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6992c6b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T14:07:11.591221Z",
     "iopub.status.busy": "2022-05-27T14:07:11.590602Z",
     "iopub.status.idle": "2022-05-27T15:07:36.462206Z",
     "shell.execute_reply": "2022-05-27T15:07:36.461297Z"
    },
    "papermill": {
     "duration": 3624.883795,
     "end_time": "2022-05-27T15:07:36.469971",
     "exception": false,
     "start_time": "2022-05-27T14:07:11.586176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 sg coffee places scraped!\n",
      "Break for 193 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 183 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 162 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 179 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 136 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 226 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 128 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 215 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 147 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 153 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 222 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 231 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 231 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 184 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 182 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 129 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 133 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 213 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 128 seconds\n",
      "60 sg coffee places scraped!\n",
      "Break for 227 seconds\n",
      "no more businesses to be scraped!\n"
     ]
    }
   ],
   "source": [
    "coffee_places_sg = []\n",
    "for i in range(50):\n",
    "   parameters_sg = {'term':'boba',\n",
    "                 'limit':50,\n",
    "                 'radius':10000,\n",
    "                 'offset':i*50,\n",
    "                 'location':'Singapore'}\n",
    "\n",
    "   response_sg = requests.get(url=end_point,params=parameters_sg,headers=headers)\n",
    "   business_search_sg = response_sg.json()\n",
    "    \n",
    "   try:\n",
    "       [biz for biz in business_search_sg['businesses']] \n",
    "   except:\n",
    "       print(\"no more businesses to be scraped!\")\n",
    "       break\n",
    "   coffee_places_sg.extend([biz for biz in business_search_sg['businesses']])\n",
    "   print(\"{} sg coffee places scraped!\".format(len(coffee_places_sg)))\n",
    "   sleep_duration = random.randint(120,240)\n",
    "   print('Break for {} seconds'.format(sleep_duration))\n",
    "   time.sleep(sleep_duration)\n",
    "    \n",
    "df = pd.DataFrame(coffee_places_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272a6ebb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T15:07:36.482967Z",
     "iopub.status.busy": "2022-05-27T15:07:36.482318Z",
     "iopub.status.idle": "2022-05-27T15:07:36.494433Z",
     "shell.execute_reply": "2022-05-27T15:07:36.493697Z"
    },
    "papermill": {
     "duration": 0.020776,
     "end_time": "2022-05-27T15:07:36.496645",
     "exception": false,
     "start_time": "2022-05-27T15:07:36.475869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('sg_boba_shops.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "275aee09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-27T15:07:36.509343Z",
     "iopub.status.busy": "2022-05-27T15:07:36.508707Z",
     "iopub.status.idle": "2022-05-27T15:07:36.525997Z",
     "shell.execute_reply": "2022-05-27T15:07:36.524907Z"
    },
    "papermill": {
     "duration": 0.026342,
     "end_time": "2022-05-27T15:07:36.528291",
     "exception": false,
     "start_time": "2022-05-27T15:07:36.501949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # new code; caveat - some stores may have updated and past reviews and ratings provided by the same user.\n",
    "# sg_store_rev = {} #to scrape reviews from sg coffee places \n",
    "# sg_store_rating = {} #to scrape ratings from sg coffee places; \n",
    "# sg_store_userid = {} #to scrape userids for reviews and ratings from sg coffee places\n",
    "# outlet_count = 0\n",
    "# count_increment = 10\n",
    "# for alias in aliases_sg:\n",
    "#     count = 0\n",
    "#     sg_url = \"https://www.yelp.com/biz/{}?osq=coffee\".format(alias)\n",
    "#     print(sg_url)\n",
    "#     sg_store = requests.get(sg_url)\n",
    "#     sg_store_soup = bs(sg_store.content, 'lxml')\n",
    "#     if sg_store_soup.find_all('span',{'itemprop':'reviewCount'}) ==[]:\n",
    "#         print(alias, \"no reviews\")\n",
    "#         outlet_count += 1\n",
    "#         print(\"{} stores out of {} stores done!\".format(outlet_count,len(aliases_sg)))\n",
    "#         pd.DataFrame({key:pd.Series(value) for key, value in sg_store_rev.items()}).to_csv('sg_store_rev.csv', index=False)\n",
    "#         pd.DataFrame({key:pd.Series(value) for key, value in sg_store_rating.items()}).to_csv('sg_store_rating.csv', index=False)\n",
    "#         pd.DataFrame({key:pd.Series(value) for key, value in sg_store_userid.items()}).to_csv('sg_store_userid.csv', index=False)\n",
    "#         sleep_duration = random.randint(15,50)\n",
    "#         print(\"Resting {} seconds...\".format(sleep_duration))\n",
    "#         print(\" \")\n",
    "#         time.sleep(sleep_duration)\n",
    "#         continue\n",
    "#     else:\n",
    "#         next_url = sg_url + \"&start=\" + str(count+count_increment)\n",
    "#         print(\"Checking to see if next page for this store {} has any reviews and ratings to scrape...\".format(next_url))\n",
    "#         print(\" \")\n",
    "#         sg_store_next_url = next_url\n",
    "#         sg_store_next_page = requests.get(sg_store_next_url)\n",
    "#         sg_store_next_page_soup = bs(sg_store_next_page.content, 'lxml')\n",
    "#         if [tag.text for tag in sg_store_next_page_soup.find_all('span',{'lang':'en'})] == []:\n",
    "#             print(\"Nope! This store only has 1 page of reviews and ratings to be scraped! Let's go!\")\n",
    "#             print(\" \")\n",
    "#             if [tag.text for tag in sg_store_soup.find_all('span',{'lang':'en'})] == []:\n",
    "#                 print(\"But...there are no english reviews...so let's move on!\")\n",
    "#                 sg_store_reviews = []\n",
    "#                 sg_store_ratings = []\n",
    "#                 sg_store_userids = []\n",
    "#                 sg_store_rev[alias] = sg_store_reviews\n",
    "#                 sg_store_rating[alias] = sg_store_ratings\n",
    "#                 sg_store_userid[alias] = sg_store_userids\n",
    "#                 print(\"Store {} has {} english reviews\".format(alias,len(sg_store_reviews)))\n",
    "#                 print(\" \")\n",
    "#                 outlet_count += 1\n",
    "#                 print(\"{} stores out of {} stores done!\".format(outlet_count,len(aliases_sg)))\n",
    "#                 print(\" \")\n",
    "#                 pd.DataFrame({key:pd.Series(value) for key, value in sg_store_rev.items()}).to_csv('sg_store_rev.csv', index=False)\n",
    "#                 pd.DataFrame({key:pd.Series(value) for key, value in sg_store_rating.items()}).to_csv('sg_store_rating.csv', index=False)\n",
    "#                 pd.DataFrame({key:pd.Series(value) for key, value in sg_store_userid.items()}).to_csv('sg_store_userid.csv', index=False)\n",
    "#                 sleep_duration = random.randint(15,50)\n",
    "#                 print(\"Resting {} seconds...\".format(sleep_duration))\n",
    "#                 print(\" \")\n",
    "#                 time.sleep(sleep_duration)\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 sg_store_reviews = [tag.text for tag in sg_store_soup.find_all('span',{'lang':'en'})]\n",
    "#                 print(\"First review of page: {}\".format(sg_store_reviews[0]))\n",
    "#                 print(\" \")\n",
    "#                 sg_store_ratings = [tag.attrs['aria-label'] for tag in sg_store_soup.find_all('div',{'aria-label':re.compile('[0-9] star rating')})[1:len(sg_store_reviews)+1]]\n",
    "#                 print(\"First rating of page: {}\".format(sg_store_ratings[0]))\n",
    "#                 print(\" \")\n",
    "#                 atags = sg_store_soup.find_all('a', href = re.compile('\\/user_details\\?userid=\\w+'))\n",
    "#                 userids = []\n",
    "#                 for tag in atags:\n",
    "#                     m = re.search('userid=(\\w+)', str(tag))\n",
    "#                     userids.append(m.group()[7:])\n",
    "#                 sg_store_userids = userids[::2]\n",
    "#                 sg_store_rev[alias] = sg_store_reviews\n",
    "#                 sg_store_rating[alias] = sg_store_ratings\n",
    "#                 sg_store_userid[alias] = sg_store_userids\n",
    "#                 print(\"Store with 1 page of reviews and ratings {} has {} reviews\".format(alias,len(sg_store_reviews)))\n",
    "#                 print(\" \")\n",
    "#                 print(\"Store with 1 page of reviews and ratings {} has {} ratings\".format(alias,len(sg_store_ratings)))\n",
    "#                 print(\" \")\n",
    "#                 print(\"Store with 1 page of reviews and ratings {} has {} userids\".format(alias,len(sg_store_userids)))\n",
    "#                 print(\" \")\n",
    "#                 outlet_count += 1\n",
    "#                 print(\"{} stores out of {} stores done!\".format(outlet_count,len(aliases_sg)))\n",
    "#                 print(\" \")\n",
    "#                 pd.DataFrame({key:pd.Series(value) for key, value in sg_store_rev.items()}).to_csv('sg_store_rev.csv', index=False)\n",
    "#                 pd.DataFrame({key:pd.Series(value) for key, value in sg_store_rating.items()}).to_csv('sg_store_rating.csv', index=False)\n",
    "#                 pd.DataFrame({key:pd.Series(value) for key, value in sg_store_userid.items()}).to_csv('sg_store_userid.csv', index=False)\n",
    "#                 sleep_duration = random.randint(15,50)\n",
    "#                 print(\"Resting {} seconds...\".format(sleep_duration))\n",
    "#                 print(\" \")\n",
    "#                 time.sleep(sleep_duration)\n",
    "#                 continue\n",
    "#         else:\n",
    "#             print(\"Next page got! But first, extracting from 1st page first:\", sg_url)\n",
    "#             print(\" \")\n",
    "#             sg_store_reviews = []\n",
    "#             sg_store_ratings = []\n",
    "#             sg_store_userids = []\n",
    "#             if [tag.text for tag in sg_store_soup.find_all('span',{'lang':'en'})] == []:\n",
    "#                 print(\"No english reviews on this page, looking at next page..\")\n",
    "#                 while [tag.text for tag in sg_store_next_page_soup.find_all('span',{'lang':'en'})] != []:\n",
    "#                     count += count_increment\n",
    "#                     print(\"Count is currently: {}\".format(count))\n",
    "#                     print(\" \")\n",
    "#                     sg_store_next_url = sg_url + '&start=' + str(count)\n",
    "#                     print(\"New current page to scrape is {}\".format(sg_store_next_url))\n",
    "#                     print(\" \")\n",
    "#                     sg_store_next_page = requests.get(sg_store_next_url)\n",
    "#                     sg_store_next_page_soup = bs(sg_store_next_page.content, 'lxml')\n",
    "#                     sg_store_next_page_reviews = [tag.text for tag in sg_store_next_page_soup.find_all('span',{'lang':'en'})]\n",
    "#                     print(\"Number of reviews on new current page: {}\".format(len(sg_store_next_page_reviews)))\n",
    "#                     print(\" \")\n",
    "#                     print(\"First review of page: {}\".format(sg_store_next_page_reviews[0]))\n",
    "#                     print(\" \")\n",
    "#                     sg_store_reviews.extend(sg_store_next_page_reviews)\n",
    "#                     print(\"Number of reviews scraped up till new current page: {}\".format(len(sg_store_reviews)))\n",
    "#                     print(\" \")\n",
    "#                     sg_store_next_page_ratings = [tag.attrs['aria-label'] for tag in sg_store_next_page_soup.find_all('div',{'aria-label':re.compile('[0-9] star rating')})[1:len(sg_store_next_page_reviews)+1]] \n",
    "#                     print(\"Number of ratings on new current page: {}\".format(len(sg_store_next_page_ratings)))\n",
    "#                     print(\" \")\n",
    "#                     print(\"First rating of page: {}\".format(sg_store_next_page_ratings[0]))\n",
    "#                     print(\" \")\n",
    "#                     sg_store_ratings.extend(sg_store_next_page_ratings)\n",
    "#                     print(\"Number of ratings scraped up till new current page: {}\".format(len(sg_store_ratings)))\n",
    "#                     print(\" \")\n",
    "                    \n",
    "#                     atags = sg_store_next_page_soup.find_all('a', href = re.compile('\\/user_details\\?userid=\\w+'))\n",
    "#                     userids = []\n",
    "#                     for tag in atags:\n",
    "#                         m = re.search('userid=(\\w+)', str(tag))\n",
    "#                         userids.append(m.group()[7:])\n",
    "#                     sg_store_userids.extend(userids[::2])\n",
    "                    \n",
    "#                     print(\"Number of userids on new current page: {}\".format(len(userids[::2])))\n",
    "#                     print(\" \")\n",
    "#                     print(\"Number of userids scraped up till new current page: {}\".format(len(sg_store_userids)))\n",
    "#                     print(\" \")\n",
    "                    \n",
    "#                     sg_store_next_url = sg_url + '&start=' + str(count+count_increment)\n",
    "#                     print(\"Looking at the next page {} to see if got more to scrape..\".format(sg_store_next_url))\n",
    "#                     print(\" \")\n",
    "#                     sg_store_next_page = requests.get(sg_store_next_url)\n",
    "#                     sg_store_next_page_soup = bs(sg_store_next_page.content, 'lxml')\n",
    "#                 print(\"Next page no more to scrape! That's it for this store!\")\n",
    "#                 print(\" \")\n",
    "#                 sg_store_rev[alias] = sg_store_reviews\n",
    "#                 sg_store_rating[alias] = sg_store_ratings\n",
    "#                 sg_store_userid[alias] = sg_store_userids\n",
    "#                 print(\"Store with more than 1 page of reviews and ratings {} has {} reviews\".format(alias,len(sg_store_reviews)))\n",
    "#                 print(\" \")\n",
    "#                 print(\"Store with more than 1 page of reviews and ratings {} has {} ratings\".format(alias,len(sg_store_ratings)))\n",
    "#                 print(\" \")\n",
    "#                 print(\"Store with more than 1 page of reviews and ratings {} has {} userids\".format(alias,len(sg_store_userids)))\n",
    "#                 print(\" \")\n",
    "#                 outlet_count += 1\n",
    "#                 print(\"{} stores out of {} stores done!\".format(outlet_count,len(aliases_sg)))\n",
    "#                 print(\" \")\n",
    "#                 pd.DataFrame({key:pd.Series(value) for key, value in sg_store_rev.items()}).to_csv('sg_store_rev.csv', index=False)\n",
    "#                 pd.DataFrame({key:pd.Series(value) for key, value in sg_store_rating.items()}).to_csv('sg_store_rating.csv', index=False)\n",
    "#                 pd.DataFrame({key:pd.Series(value) for key, value in sg_store_userid.items()}).to_csv('sg_store_userid.csv', index=False)\n",
    "#                 sleep_duration = random.randint(15,50)\n",
    "#                 print(\"Resting {} seconds...\".format(sleep_duration))\n",
    "#                 print(\" \")\n",
    "#                 time.sleep(sleep_duration)\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 sg_store_reviews = [tag.text for tag in sg_store_soup.find_all('span',{'lang':'en'})]\n",
    "#                 print(\"Number of reviews extracted from {} is {}\".format(sg_url,len(sg_store_reviews)))\n",
    "#                 print(\" \")\n",
    "#                 print(\"First review of page: {}\".format(sg_store_reviews[0]))\n",
    "#                 print(\" \")\n",
    "#                 sg_store_ratings = [tag.attrs['aria-label'] for tag in sg_store_soup.find_all('div',{'aria-label':re.compile('[0-9] star rating')})[1:len(sg_store_reviews)+1]]\n",
    "#                 print(\"Number of ratings extracted from {} is {}\".format(sg_url,len(sg_store_ratings)))\n",
    "#                 print(\" \")\n",
    "#                 print(\"First rating of page: {}\".format(sg_store_ratings[0]))\n",
    "#                 print(\" \")\n",
    "#                 atags = sg_store_soup.find_all('a', href = re.compile('\\/user_details\\?userid=\\w+'))\n",
    "#                 userids = []\n",
    "#                 for tag in atags:\n",
    "#                     m = re.search('userid=(\\w+)', str(tag))\n",
    "#                     userids.append(m.group()[7:])\n",
    "#                 sg_store_userids = userids[::2]\n",
    "#                 print(\"Number of userids extracted from {} is {}\".format(sg_url,len(sg_store_userids)))\n",
    "#                 print(\" \")\n",
    "                \n",
    "                \n",
    "#                 while [tag.text for tag in sg_store_next_page_soup.find_all('span',{'lang':'en'})] != []:\n",
    "#                     print(\"Next page got some more! Thus will increase count...\")\n",
    "#                     print(\" \")\n",
    "#                     count += count_increment\n",
    "#                     print(\"Count is currently: {}\".format(count))\n",
    "#                     print(\" \")\n",
    "#                     sg_store_next_url = sg_url + '&start=' + str(count)\n",
    "#                     print(\"New current page to scrape is {}\".format(sg_store_next_url))\n",
    "#                     print(\" \")\n",
    "#                     sg_store_next_page = requests.get(sg_store_next_url)\n",
    "#                     sg_store_next_page_soup = bs(sg_store_next_page.content, 'lxml')\n",
    "#                     sg_store_next_page_reviews = [tag.text for tag in sg_store_next_page_soup.find_all('span',{'lang':'en'})]\n",
    "#                     print(\"Number of reviews on new current page: {}\".format(len(sg_store_next_page_reviews)))\n",
    "#                     print(\" \")\n",
    "#                     print(\"First review of page: {}\".format(sg_store_next_page_reviews[0]))\n",
    "#                     print(\" \")\n",
    "#                     sg_store_reviews.extend(sg_store_next_page_reviews)\n",
    "#                     print(\"Number of reviews scraped up till new current page: {}\".format(len(sg_store_reviews)))\n",
    "#                     print(\" \")\n",
    "#                     sg_store_next_page_ratings = [tag.attrs['aria-label'] for tag in sg_store_next_page_soup.find_all('div',{'aria-label':re.compile('[0-9] star rating')})[1:len(sg_store_next_page_reviews)+1]] \n",
    "#                     print(\"Number of ratings on new current page: {}\".format(len(sg_store_next_page_ratings)))\n",
    "#                     print(\" \")\n",
    "#                     print(\"First rating of page: {}\".format(sg_store_next_page_ratings[0]))\n",
    "#                     print(\" \")\n",
    "#                     sg_store_ratings.extend(sg_store_next_page_ratings)\n",
    "#                     print(\"Number of ratings scraped up till new current page: {}\".format(len(sg_store_ratings)))\n",
    "#                     print(\" \")\n",
    "                    \n",
    "#                     atags = sg_store_next_page_soup.find_all('a', href = re.compile('\\/user_details\\?userid=\\w+'))\n",
    "#                     userids = []\n",
    "#                     for tag in atags:\n",
    "#                         m = re.search('userid=(\\w+)', str(tag))\n",
    "#                         userids.append(m.group()[7:])\n",
    "#                     sg_store_userids.extend(userids[::2])\n",
    "#                     print(\"Number of userids on new current page: {}\".format(len(userids[::2])))\n",
    "#                     print(\" \")\n",
    "#                     print(\"Number of userids scraped up till new current page: {}\".format(len(sg_store_userids)))\n",
    "#                     print(\" \")\n",
    "            \n",
    "#                     sg_store_next_url = sg_url + '&start=' + str(count+count_increment)\n",
    "#                     print(\"Looking at the next page {} to see if got more to scrape..\".format(sg_store_next_url))\n",
    "#                     print(\" \")\n",
    "#                     sg_store_next_page = requests.get(sg_store_next_url)\n",
    "#                     sg_store_next_page_soup = bs(sg_store_next_page.content, 'lxml')\n",
    "#                 print(\"Next page no more to scrape! That's it for this store!\")\n",
    "#                 print(\" \")\n",
    "#                 sg_store_rev[alias] = sg_store_reviews\n",
    "#                 sg_store_rating[alias] = sg_store_ratings\n",
    "#                 sg_store_userid[alias] = sg_store_userids\n",
    "#                 print(\"Store with more than 1 page of reviews and ratings {} has {} reviews\".format(alias,len(sg_store_reviews)))\n",
    "#                 print(\" \")\n",
    "#                 print(\"Store with more than 1 page of reviews and ratings {} has {} ratings\".format(alias,len(sg_store_ratings)))\n",
    "#                 print(\" \")\n",
    "#                 print(\"Store with more than 1 page of reviews and ratings {} has {} userids\".format(alias,len(sg_store_userids)))\n",
    "#                 print(\" \")\n",
    "#                 outlet_count += 1\n",
    "#                 print(\"{} stores out of {} stores done!\".format(outlet_count,len(aliases_sg)))\n",
    "#                 print(\" \")\n",
    "#                 pd.DataFrame({key:pd.Series(value) for key, value in sg_store_rev.items()}).to_csv('sg_store_rev.csv', index=False)\n",
    "#                 pd.DataFrame({key:pd.Series(value) for key, value in sg_store_rating.items()}).to_csv('sg_store_rating.csv', index=False)\n",
    "#                 pd.DataFrame({key:pd.Series(value) for key, value in sg_store_userid.items()}).to_csv('sg_store_userid.csv', index=False)\n",
    "#                 sleep_duration = random.randint(15,50)\n",
    "#                 print(\"Resting {} seconds...\".format(sleep_duration))\n",
    "#                 print(\" \")\n",
    "#                 time.sleep(sleep_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f1211",
   "metadata": {
    "papermill": {
     "duration": 0.004884,
     "end_time": "2022-05-27T15:07:36.538391",
     "exception": false,
     "start_time": "2022-05-27T15:07:36.533507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3635.698517,
   "end_time": "2022-05-27T15:07:37.171995",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-27T14:07:01.473478",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
